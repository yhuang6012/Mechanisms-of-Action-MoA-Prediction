{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008471,
     "end_time": "2020-10-18T22:35:40.439323",
     "exception": false,
     "start_time": "2020-10-18T22:35:40.430852",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Multi Input ResNet Model\n",
    "\n",
    "\n",
    "## This notebook is a python/tensorflow version of [this notebook](https://www.kaggle.com/demetrypascal/2heads-deep-resnets-pipeline-smoothing). Please upvote the original as well if you find this work useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-18T22:35:40.461588Z",
     "iopub.status.busy": "2020-10-18T22:35:40.460871Z",
     "iopub.status.idle": "2020-10-18T22:35:46.706662Z",
     "shell.execute_reply": "2020-10-18T22:35:46.707142Z"
    },
    "papermill": {
     "duration": 6.260524,
     "end_time": "2020-10-18T22:35:46.707353",
     "exception": false,
     "start_time": "2020-10-18T22:35:40.446829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras import layers,regularizers,Sequential,Model,backend,callbacks,optimizers,metrics,losses\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import json\n",
    "# sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-18T22:35:46.733597Z",
     "iopub.status.busy": "2020-10-18T22:35:46.732951Z",
     "iopub.status.idle": "2020-10-18T22:35:53.254327Z",
     "shell.execute_reply": "2020-10-18T22:35:53.253767Z"
    },
    "papermill": {
     "duration": 6.538737,
     "end_time": "2020-10-18T22:35:53.254467",
     "exception": false,
     "start_time": "2020-10-18T22:35:46.715730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import train data, drop sig_id, cp_type\n",
    "\n",
    "train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "non_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\n",
    "train_features = train_features.drop(['sig_id','cp_type','cp_dose','cp_time'],axis=1)\n",
    "train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "train_targets_scored = train_targets_scored.drop('sig_id',axis=1)\n",
    "labels_train = train_targets_scored.values\n",
    "\n",
    "# Drop training data with ctl vehicle\n",
    "\n",
    "train_features = train_features.iloc[non_ctl_idx]\n",
    "labels_train = labels_train[non_ctl_idx]\n",
    "\n",
    "# Import test data\n",
    "\n",
    "test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "test_features = test_features.drop(['sig_id','cp_dose','cp_time'],axis=1)\n",
    "\n",
    "# Import predictors from public kernel\n",
    "\n",
    "json_file_path = '../input/t-test-pca-rfe-logistic-regression/main_predictors.json'\n",
    "\n",
    "with open(json_file_path, 'r') as j:\n",
    "    predictors = json.loads(j.read())\n",
    "    predictors = predictors['start_predictors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T22:35:53.285293Z",
     "iopub.status.busy": "2020-10-18T22:35:53.284584Z",
     "iopub.status.idle": "2020-10-18T22:35:53.287818Z",
     "shell.execute_reply": "2020-10-18T22:35:53.287383Z"
    },
    "papermill": {
     "duration": 0.025372,
     "end_time": "2020-10-18T22:35:53.287937",
     "exception": false,
     "start_time": "2020-10-18T22:35:53.262565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create g-mean, c-mean, genes_pca (2 components), cells_pca (all components)\n",
    "\n",
    "cs = train_features.columns.str.contains('c-')\n",
    "gs = train_features.columns.str.contains('g-')\n",
    "\n",
    "def preprocessor(train,test):\n",
    "    \n",
    "    # PCA\n",
    "    \n",
    "    n_gs = 2 # No of PCA comps to include\n",
    "    n_cs = 100 # No of PCA comps to include\n",
    "    \n",
    "    pca_cs = PCA(n_components = n_cs)\n",
    "    pca_gs = PCA(n_components = n_gs)\n",
    "\n",
    "    train_pca_gs = pca_gs.fit_transform(train[:,gs])\n",
    "    train_pca_cs = pca_cs.fit_transform(train[:,cs])\n",
    "    test_pca_gs = pca_gs.transform(test[:,gs])\n",
    "    test_pca_cs = pca_cs.transform(test[:,cs])\n",
    "    \n",
    "    # c-mean, g-mean\n",
    "    \n",
    "    train_c_mean = train[:,cs].mean(axis=1)\n",
    "    test_c_mean = test[:,cs].mean(axis=1)\n",
    "    train_g_mean = train[:,gs].mean(axis=1)\n",
    "    test_g_mean = test[:,gs].mean(axis=1)\n",
    "    \n",
    "    # Append Features\n",
    "    \n",
    "    train = np.concatenate((train,train_pca_gs,train_pca_cs,train_c_mean[:,np.newaxis]\n",
    "                            ,train_g_mean[:,np.newaxis]),axis=1)\n",
    "    test = np.concatenate((test,test_pca_gs,test_pca_cs,test_c_mean[:,np.newaxis],\n",
    "                           test_g_mean[:,np.newaxis]),axis=1)\n",
    "    \n",
    "    # Scaler for numerical values\n",
    "\n",
    "    # Scale train data\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    train = scaler.fit_transform(train)\n",
    "\n",
    "    # Scale Test data\n",
    "    test = scaler.transform(test)\n",
    "    \n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T22:35:53.312040Z",
     "iopub.status.busy": "2020-10-18T22:35:53.310293Z",
     "iopub.status.idle": "2020-10-18T22:35:53.312691Z",
     "shell.execute_reply": "2020-10-18T22:35:53.313152Z"
    },
    "papermill": {
     "duration": 0.017566,
     "end_time": "2020-10-18T22:35:53.313275",
     "exception": false,
     "start_time": "2020-10-18T22:35:53.295709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_labels = train_targets_scored.shape[1]\n",
    "n_train = train_features.shape[0]\n",
    "n_test = test_features.shape[0]\n",
    "\n",
    "\n",
    "# Prediction Clipping Thresholds\n",
    "\n",
    "p_min = 0.0005\n",
    "p_max = 0.9995\n",
    "\n",
    "# OOF Evaluation Metric with clipping and no label smoothing\n",
    "\n",
    "def logloss(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n",
    "    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T22:35:53.344090Z",
     "iopub.status.busy": "2020-10-18T22:35:53.343371Z",
     "iopub.status.idle": "2020-10-18T22:35:53.346117Z",
     "shell.execute_reply": "2020-10-18T22:35:53.346565Z"
    },
    "papermill": {
     "duration": 0.025632,
     "end_time": "2020-10-18T22:35:53.346677",
     "exception": false,
     "start_time": "2020-10-18T22:35:53.321045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(n_features, n_features_2, n_labels, label_smoothing = 0.0005):    \n",
    "    input_1 = layers.Input(shape = (n_features,), name = 'Input1')\n",
    "    input_2 = layers.Input(shape = (n_features_2,), name = 'Input2')\n",
    "\n",
    "    head_1 = Sequential([\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(512, activation=\"elu\"), \n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(256, activation = \"elu\")\n",
    "        ],name='Head1') \n",
    "\n",
    "    input_3 = head_1(input_1)\n",
    "    input_3_concat = layers.Concatenate()([input_2, input_3])\n",
    "\n",
    "    head_2 = Sequential([\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(512, \"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(512, \"elu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(256, \"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(256, \"elu\")\n",
    "        ],name='Head2')\n",
    "\n",
    "    input_4 = head_2(input_3_concat)\n",
    "    input_4_avg = layers.Average()([input_3, input_4]) \n",
    "\n",
    "    head_3 = Sequential([\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(256, kernel_initializer='lecun_normal', activation='selu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(n_labels, kernel_initializer='lecun_normal', activation='selu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(n_labels, activation=\"sigmoid\")\n",
    "        ],name='Head3')\n",
    "\n",
    "    output = head_3(input_4_avg)\n",
    "\n",
    "\n",
    "    model = Model(inputs = [input_1, input_2], outputs = output)\n",
    "    model.compile(optimizer='adam', loss=losses.BinaryCrossentropy(label_smoothing=label_smoothing), metrics=logloss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import os\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.compat.v1.random.set_random_seed(seed)\n",
    "    \n",
    "# seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T22:35:53.380367Z",
     "iopub.status.busy": "2020-10-18T22:35:53.369981Z",
     "iopub.status.idle": "2020-10-18T23:17:32.324871Z",
     "shell.execute_reply": "2020-10-18T23:17:32.324228Z"
    },
    "papermill": {
     "duration": 2498.97043,
     "end_time": "2020-10-18T23:17:32.324988",
     "exception": false,
     "start_time": "2020-10-18T22:35:53.354558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate Seeds\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "# seeds = np.random.randint(0,100,size=n_seeds)\n",
    "seeds = [0,1,2,3,4,5,6]\n",
    "n_seeds = len(seeds)\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "n_folds = 7\n",
    "y_oof = np.zeros((n_train,n_labels))\n",
    "y_pred = np.zeros((n_test,n_labels))\n",
    "oof = tf.constant(0.0)\n",
    "hists = []\n",
    "for seed in seeds:\n",
    "    seed_everything(seed)\n",
    "    fold = 0\n",
    "    kf = KFold(n_splits=n_folds,shuffle=True,random_state=seed)\n",
    "    for train, test in kf.split(train_features):\n",
    "        X_train, X_test = preprocessor(train_features.iloc[train].values,\n",
    "                                       train_features.iloc[test].values)\n",
    "        _,data_test = preprocessor(train_features.iloc[train].values,\n",
    "                                   test_features.drop('cp_type',axis=1).values)\n",
    "        X_train_2 = train_features.iloc[train][predictors].values\n",
    "        X_test_2 = train_features.iloc[test][predictors].values\n",
    "        data_test_2 = test_features[predictors].values\n",
    "        y_train = labels_train[train]\n",
    "        y_test = labels_train[test]\n",
    "        n_features = X_train.shape[1]\n",
    "        n_features_2 = X_train_2.shape[1]\n",
    "\n",
    "        model = build_model(n_features, n_features_2, n_labels)\n",
    "        \n",
    "        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.1, patience=2, mode='min', min_lr=1E-5)\n",
    "        early_stopping = callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=10, mode='min',restore_best_weights=True)\n",
    "\n",
    "        hist = model.fit([X_train,X_train_2],y_train, batch_size=128, epochs=192,verbose=0,validation_data = ([X_test,X_test_2],y_test),\n",
    "                         callbacks=[reduce_lr, early_stopping])\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "        hists.append(hist)\n",
    "        \n",
    "        # Save Model\n",
    "        \n",
    "        model.save_weights(f\"FOLD{fold}_SEED{seed}.h5\")\n",
    "\n",
    "        # OOF Score\n",
    "        y_val = model.predict([X_test,X_test_2])\n",
    "        y_oof[test,:] = y_val\n",
    "        oof += logloss(tf.constant(y_test,dtype=tf.float32),tf.constant(y_val,dtype=tf.float32))/(n_folds*n_seeds)\n",
    "\n",
    "        # Run prediction\n",
    "        y_pred += model.predict([data_test,data_test_2])/(n_folds*n_seeds)\n",
    "\n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21948, 206)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T23:17:32.347044Z",
     "iopub.status.busy": "2020-10-18T23:17:32.346181Z",
     "iopub.status.idle": "2020-10-18T23:17:33.037827Z",
     "shell.execute_reply": "2020-10-18T23:17:33.037159Z"
    },
    "papermill": {
     "duration": 0.704091,
     "end_time": "2020-10-18T23:17:33.037937",
     "exception": false,
     "start_time": "2020-10-18T23:17:32.333846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "\n",
    "# tf.keras.utils.plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../input/lish-moa/sample_submission.csv')\n",
    "\n",
    "target_cols = sub.columns.tolist()\n",
    "target_cols.remove('sig_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_targets_scored_cv = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "\n",
    "valid_results = train_targets_scored_cv.loc[non_ctl_idx]\n",
    "valid_results[target_cols] = y_oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV log_loss:  0.015328379744572528\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "valid_results = train_targets_scored[['sig_id']].merge(valid_results, on='sig_id', how='left').fillna(0)\n",
    "\n",
    "y_true_2 = train_targets_scored[target_cols].values\n",
    "y_pred_2 = valid_results[target_cols].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(target_cols)):\n",
    "    score_ = log_loss(y_true_2[:, i], y_pred_2[:, i])\n",
    "    score += score_ / len(target_cols)\n",
    "    \n",
    "print(\"CV log_loss: \", score)    \n",
    "\n",
    "# CV log_loss:  0.015227376680156588 10fold\n",
    "# CV log_loss:  0.01578390613611236   3fold\n",
    "# CV log_loss:  0.01577570647725881   3fold  seed fixed but score not the same\n",
    "# CV log_loss:  0.015414727316422419  7fold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_results.to_csv('oof_model10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23814, 207)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-18T23:17:33.691058Z",
     "iopub.status.busy": "2020-10-18T23:17:33.690451Z",
     "iopub.status.idle": "2020-10-18T23:17:36.328420Z",
     "shell.execute_reply": "2020-10-18T23:17:36.327107Z"
    },
    "papermill": {
     "duration": 2.656582,
     "end_time": "2020-10-18T23:17:36.328572",
     "exception": false,
     "start_time": "2020-10-18T23:17:33.671990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate submission file, Clip Predictions\n",
    "\n",
    "sub.loc[:,target_cols] = y_pred\n",
    "\n",
    "# Set ctl_vehicle to 0\n",
    "sub.loc[test_features['cp_type'] == 'ctl_vehicle',target_cols] = 0\n",
    "\n",
    "# Save Submission\n",
    "sub.to_csv('submission_model10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare = pd.read_csv('../pubsub/submission_0.1854.csv')\n",
    "\n",
    "\n",
    "# corrlist = []\n",
    "# for col in compare.columns:\n",
    "#     if col!='sig_id':\n",
    "#         corrlist.append([col,np.corrcoef([compare[col],sub[col]])[0,1]])\n",
    "# corrlist = pd.DataFrame(corrlist)\n",
    "# corrlist[1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "papermill": {
   "duration": 2521.443341,
   "end_time": "2020-10-18T23:17:37.953054",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-18T22:35:36.509713",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
